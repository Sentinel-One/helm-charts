{{- if not (mustRegexMatch "^(kubernetes|openshift|talos|serverless|bottlerocket)$" .Values.configuration.platform.type) -}}
{{ fail "configuration.platform.type must be one of: kubernetes, openshift, serverless, bottlerocket" }}
{{- end -}}
{{- if not (mustRegexMatch "^(info|error|warning|debug|trace)$" .Values.configuration.env.helper.log_level) -}}
{{ fail "configuration.env.helper.log_level must be one of: info, error, warning, debug, trace" }}
{{- end -}}
{{- if not (mustRegexMatch "^(info|error|warning|debug|trace)$" .Values.configuration.env.agent.log_level) -}}
{{ fail "configuration.env.agent.log_level must be one of: info, error, warning, debug, trace" }}
{{- end -}}

Installed the SentinelOne Agent Helm chart to your cluster.

Configuration options used to deploy:

* Cluster name: '{{ .Values.configuration.cluster.name }}'.
  This will be reported as the "Cluster Name" in your console, in the details of every node of this cluster.
* Platform support enabled for: '{{ .Values.configuration.platform.type }}'

{{- if include "site_key.secret.create" . }}
* A site-key secret named '{{ include "site_key.secret.name" . }}' was CREATED (or overwritten if it already existed).
{{- else if include "site_key.secret.name" . }}
* The name of the PRE-EXISTING site-key secret to use is '{{ include "site_key.secret.name" . }}'
{{- else }}
* Neither a site-key secret name nor a value to create the secret with was provided. The agents will work in OFFLINE mode.
{{- end }}
{{- if .Values.configuration.custom_ca }}
{{- if .Values.configuration.custom_ca_path }}
* A custom CA certificate will be loaded into the agent and helper image. These certificates were loaded with "--set-files". If the list is empty, check your arguments.
{{- else if .Values.configuration.custom_ca_name }}
* A custom CA certificate will be loaded into the agent and helper image from PRE-EXISTING secret.
{{- else }}
* A custom CA certificate will be loaded into the agent and agent image. These files were loaded. If the list is empty, then you probably did not copy the certificate files to "files/*.pem".
{{- end }}
{{- $agentCerts := fromYaml (include "agent.certificates" .) -}}
{{- range $cert := $agentCerts.certificates }}
  -> {{ $cert.name }}
{{- end }}
{{- end }}
{{- if .Values.configuration.proxy }}
* A proxy will be used between AGENTS and MANAGEMENT: '{{ .Values.configuration.proxy }}'
{{- else }}
* NO proxy will be used between AGENTS and MANAGEMENT.
{{- end }}
{{- if .Values.configuration.dv_proxy }}
* A proxy will be used between AGENTS and DEEP-VISIBILITY: '{{ .Values.configuration.dv_proxy }}'
{{- else }}
* NO proxy will be used between AGENTS and DEEP-VISIBILITY.
{{- end }}
{{- if and .Values.configuration.platform.gke.autopilot .Values.configuration.env.agent.persistent_dir }}
* Custom persistent directory is unsupported in GKE autopilot and will be overridden by a default path.
{{- end }}
* The images will be pulled from:
{{- $agentTag := .Values.configuration.tag.agent -}}
{{- $helperTag := default .Values.configuration.tag.agent .Values.configuration.tag.helper -}}
{{- if .Values.configuration.image.agent }}
  - Agent:  location:   '{{ .Values.configuration.image.agent }}'
  {{- $agentTag = last (regexSplit ":" .Values.configuration.image.agent -1) -}}
{{- else }}
  - Agent:  repository: '{{ .Values.configuration.repositories.agent }}', tag: '{{ $agentTag }}'
{{- end }}
{{- if .Values.configuration.image.helper }}
  - Helper: location:   '{{ .Values.configuration.image.helper }}'
  {{- $helperTag = last (regexSplit ":" .Values.configuration.image.helper -1) -}}
{{- else }}
  - Helper: repository: '{{ .Values.configuration.repositories.helper }}', tag: '{{ $helperTag }}'
{{- end }}
{{- if ne ($agentTag | toString) ($helperTag | toString) }}
  !!! Agent and helper tag are different; this is not normally desirable. Please use matching versions unless instructed otherwise by technical support. !!!
{{- end }}

If the pods do not start, please check if:

* The images can be pulled. This requires that:
  - The image pull secret '{{ .Values.secrets.imagePullSecret }}' exists in the namespace of this chart
  - The registries for Agent and Helper images can be reached
{{- if or .Values.agent.priorityClassName .Values.helper.priorityClassName }}
  - The values specified for the priorityClassName fields are valid
{{- end }}

* Agent pods and kubernetes API server can reach the helper via HTTPS
{{- if not (include "helper.secret.create" .) }}
* You had opted to use an EXISTING helper certificate signed by kubernetes CA.
  Make sure that in namespace '{{ .Release.Namespace }}', the secret '{{ include "helper.secret.name" . }}' ACTUALLY exists, or agents and API server will be unable to reach the helper.
{{- end }}

If the agents do not show up in the console, please check that:

* If a proxy needs to be used, it is configured and reachable for the agent pods.
* If an on-prem console with a certificate signed by a private CA is used, a CA certificate was supplied.
  If you supplied a CA certificate, check that it is stored in the namespace of this chart in your cluster as 'ca-certificate'
* Agent pods can reach the console via HTTPS
{{- if not (include "site_key.secret.create" .) }}
* You had opted to use an EXISTING site key. Make sure that in namespace '{{ .Release.Namespace }}', the secret '{{ include "site_key.secret.name" . }}' ACTUALLY exists, or agents will be unable to register.
{{- end -}}

